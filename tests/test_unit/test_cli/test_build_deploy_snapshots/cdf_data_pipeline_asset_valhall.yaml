DataSet:
- description: Asset data for oid
  externalId: ds_asset_oid
  metadata:
    consoleSource: '{"names": ["workmate"]}'
    rawTables: '[{"databaseName": "asset_oid_workmate", "tableName": "assets"}]'
    transformations: '[{"externalId": "tr_asset_oid_workmate_asset_hierarchy", "type":
      "Transformations"}]'
  name: asset:oid
  writeProtected: false
Database:
- name: asset_oid_workmate
ExtractionPipeline:
- createdBy: unknown
  dataSetId: 5942118280324318086
  description: Asset source extraction pipeline with configuration for DB extractor
    reading data from oid:workmate
  documentation: "The DB Extractor is a general database extractor that connects to\
    \ a database, executes one or several queries and sends the result to CDF RAW.\n\
    \nThe extractor connects to a database over ODBC, which means that you need an\
    \ ODBC driver for your database. If you are running the Docker version of the\
    \ extractor, ODBC drivers for MySQL, MS SQL, PostgreSql and Oracle DB are preinstalled\
    \ in the image. See the example config for details on connection strings for these.\
    \ If you are running the Windows exe version of the extractor, you must provide\
    \ an ODBC driver yourself. These are typically provided by the database vendor.\n\
    \nFurther documentation is available [here](./docs/documentation.md)\n\nFor information\
    \ on development, consider the following guides:\n\n * [Development guide](guides/development.md)\n\
    \ * [Release guide](guides/release.md)"
  externalId: ep_src_asset_oid_workmate
  name: src:asset:oid:workmate
  rawTables:
  - dbName: asset_oid_workmate
    tableName: assets
  source: workmate
ExtractionPipelineConfig:
- config: "logger:\n  console:\n    level: INFO\n  file:\n    level: INFO\n    path:\
    \ \"file.log\"\n# List of databases\ndatabases:\n  - type: odbc\n    name: postgres\n\
    \    connection-string: \"DSN={MyPostgresDsn}\"\n# List of queries\nqueries:\n\
    \  - name: test-postgres\n    database: postgres\n    query: >\n      SELECT\n\
    \n        *\n      FROM\n\n        mytable\n      WHERE\n\n        {incremental_field}\
    \ >= '{start_at}'\n      ORDER BY\n\n        {incremental_field} ASC\n    incremental-field:\
    \ \"id\"\n    initial-start: 0\n    destination:\n      type: raw\n      database:\
    \ \"db-extractor\"\n      table: \"postgres\"\n    primary-key: \"{id}\"\n"
  description: DB extractor config reading data from oid:workmate
  externalId: ep_src_asset_oid_workmate
Group:
- capabilities:
  - rawAcl:
      actions:
      - READ
      - WRITE
      scope:
        tableScope:
          dbsToTables:
            asset_oid_workmate:
              tables: []
  - extractionConfigsAcl:
      actions:
      - READ
      scope:
        extractionPipelineScope:
          ids:
          - 1471801203478289213
  metadata:
    origin: cdf-project-templates
  name: gp_asset_oid_extractor
  sourceId: <change_me>
- capabilities:
  - rawAcl:
      actions:
      - READ
      - WRITE
      scope:
        tableScope:
          dbsToTables:
            asset_oid_workmate:
              tables: []
  - transformationsAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - sessionsAcl:
      actions:
      - LIST
      - CREATE
      - DELETE
      scope:
        all: {}
  - assetsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 5942118280324318086
  metadata:
    origin: cdf-project-templates
  name: gp_asset_oid_processing
  sourceId: <change_me>
- capabilities:
  - assetsAcl:
      actions:
      - READ
      scope:
        datasetScope:
          ids:
          - 5942118280324318086
  metadata:
    origin: cdf-project-templates
  name: gp_asset_oid_read
  sourceId: <change_me>
Table:
- createdTime: 1
  name: assets
Transformation:
- conflictMode: upsert
  dataSetId: 5942118280324318086
  destination:
    type: asset_hierarchy
  destinationOidcCredentials:
    audience: https://bluefield.cognitedata.com
    cdfProjectName: pytest-project
    clientId: dummy
    clientSecret: dummy
    scopes: https://bluefield.cognitedata.com/.default
    tokenUri: dummy
  externalId: tr_asset_oid_workmate_asset_hierarchy
  ignoreNullFields: true
  isPublic: true
  name: asset:oid:workmate:asset_hierarchy
  query: "--\n-- Create Asset Hierarchy using Transformation\n--\n-- Input data from\
    \ RAW DB table (using example data)\n--\n-- Root node has parentExternal id =\
    \ ''\n-- Transformation is connected to asset data set\n-- All metadata expect\
    \ selected fileds are added to metadata\n--\nSELECT \n  externalId           \
    \           as externalId,\n  if(parentExternalId is null, \n     '', \n     parentExternalId)\
    \            as parentExternalId,\n  tag                             as name,\n\
    \  sourceDb                        as source,\n  description,\n  dataset_id('ds_asset_oid')\
    \     as dataSetId,\n  to_metadata_except(\n    array(\"sourceDb\", \"parentExternalId\"\
    , \"description\"), *) \n                                  as metadata\nFROM \n\
    \  `asset_oid_workmate`.`assets`\n"
  sourceOidcCredentials:
    audience: https://bluefield.cognitedata.com
    cdfProjectName: pytest-project
    clientId: dummy
    clientSecret: dummy
    scopes: https://bluefield.cognitedata.com/.default
    tokenUri: dummy
TransformationSchedule:
- externalId: tr_asset_oid_workmate_asset_hierarchy
  interval: 7 * * * *
  isPaused: true
