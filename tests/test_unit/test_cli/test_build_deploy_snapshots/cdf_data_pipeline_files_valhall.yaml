DataSet:
- description: Asset data for oid
  externalId: ds_asset_oid
  metadata:
    consoleSource: '{"names": ["fileshare"]}'
    rawTables: '[{"databaseName": "asset_oid_fileshare", "tableName": "assets"}]'
    transformations: '[{"externalId": "tr_asset_oid_fileshare_asset_hierarchy", "type":
      "Transformations"}]'
  name: asset:oid
  writeProtected: false
- description: File data for oid
  externalId: ds_files_oid
  metadata:
    consoleSource: '{"names": ["fileshare"]}'
    rawTables: '[{"databaseName": "files_oid_fileshare", "tableName": "files_metadata"}]'
    transformations: '[{"externalId": "tr_files_oid_fileshare_file_metadata", "type":
      "Transformations"}]'
  name: files:oid
  writeProtected: false
ExtractionPipeline:
- createdBy: unknown
  dataSetId: 42
  description: Annotation of P&ID documents from file source  oid:fileshare
  documentation: "# Contextualization / Annotation of P&ID Documents\n\nThe P&ID Annotation\
    \ runs as a process in CDF Functions. Process triggers on new and updated documents\
    \ based on update timestamp for document.\n\nExtracts all tags in P&ID that matches\
    \ tags from Asset Hierarchy and creates CDF annotations used for linking found\
    \ objects in document to other resource types in CDF.\n\nDefault configuration\
    \ is provided as part of the deployment, but can be overwritten by configuration\
    \ provided in configuration section below\n```\n # if True, run without writing\
    \ any annotations to CDF\n debug: False\n # if True run annotation on all found\
    \ documents, if False only run on document not updated since last  annotation\n\
    \ runAll: False\n # Number of document from each asset to annotate -1 = All, else\
    \ number is used\n docLimit: -1\n # Data set to read documents / P&ID from + update\
    \ with annotated timestamp when done\n docDataSetExtId: ds_files_oid\n # In document\
    \ metadata, where is location of property for P&ID type documents\n docTypeMetaCol:\
    \ \"doc_type\"\n # Document type for P&ID type documents\n pAndIdDocType: \"PNID\"\
    \n # List of externalId for root assets to be used for annotation of documents.\
    \ Documents are also extracted based on the root asset ID\n assetRootExtIds: [WMT:VAL]\n\
    \ # Number between 0 and 1, indication of strict the matching should be 1 = only\
    \ exact matches\n matchTreshold: 0.85\n```\n"
  externalId: ep_ctx_files_oid_fileshare_pandid_annotation
  name: ctx:files:oid:fileshare:pandid_annotation
  source: fileshare
- createdBy: unknown
  dataSetId: 42
  description: File source extraction pipeline with configuration for File extractor
    reading data from oid:fileshare
  documentation: "The Cognite File Extractor is a generic file extractor that can\
    \ connect to and extract file from a host of different sources. \n\nThe currently\
    \ supported sources are: \n  * Local files \n  * File Transfer Protocol (FTP)\n\
    \  * Secure File Transfer Protocol (SFTP)\n  * File Transfer Protocol over SSH\
    \ (FTPS)\n  * Documents and files in SharePoint Online \n  * Files in Google Cloud\
    \ Storage \n\n\nThe extractor is provided with template configuration files for\
    \ all mentioned source systems. To set up the file extractor against your source\
    \ system, pick the config template that fits your setup and adjust parameters\
    \ as you see fit.\n"
  externalId: ep_src_files_oid_fileshare
  name: src:files:oid:fileshare
  rawTables:
  - dbName: files_oid_fileshare
    tableName: files_metadata
  source: fileshare
ExtractionPipelineConfig:
- config: "data:\n  debug: False\n  runAll: False\n  docLimit: -1\n  docDataSetExtId:\
    \ ds_files_oid\n  docTypeMetaCol: \"doc_type\"\n  pAndIdDocType: \"PNID\"\n  assetRootExtIds:\
    \ [WMT:VAL]\n  matchThreshold: 0.85\n \n"
  externalId: ep_ctx_files_oid_fileshare_pandid_annotation
- config: "logger:\n  # Logging to console/terminal. Remove or comment out to disable\
    \ terminal\n  # logging\n  console:\n    level: INFO\n\n  # Logging to file. Include\
    \ to enable file logging\n  file:\n    level: INFO\n    path: \"c:/tmp/file.log\"\
    \n\ncognite:\n  # Data set to attach uploaded files to. Either use CDF IDs (integers)\
    \ or\n  # user-given external ids (strings)\n  data-set:\n    external-id: ds-files:valhall\n\
    \n# Information about files to extract\nfiles:\n  extensions:\n    - .pdf\n\n\
    \  # Information about file provider\n  file-provider:\n    type: local\n\n  \
    \  # For local files: Absolute or relative path to directory to watch\n    path:\
    \ c:/tmp/files"
  externalId: ep_src_files_oid_fileshare
Function:
- cpu: 0.6
  description: Contextualization of P&ID files creating annotations
  envVars:
    CDF_ENV: dev
    ENV_TYPE: dev
  externalId: fn_context_files_oid_fileshare_annotation
  fileId: -1
  functionPath: handler.py
  metadata:
    cdf-toolkit-function-hash: d7107f17192e7e79ed7919cbf70c628e18e0d966da9a99d85f05eae8dc164af0
    version: 0.0.1
  name: context:files:oid:fileshare:annotation
  owner: Anonymous
  runtime: py311
- cpu: 0.6
  description: Workflow scheduler for Contextualization of P&ID files creating annotations
  envVars:
    CDF_ENV: dev
    ENV_TYPE: dev
  externalId: fn_workflow_files_oid_fileshare_annotation
  fileId: -1
  functionPath: handler.py
  metadata:
    cdf-toolkit-function-hash: c59aa6c790b492bdf98483455c9774921ff7e3b1167e0b96c7951c6d13140f14
    cdf-toolkit-secret-hash: 3c751aaf4bd17e8ea97201022e5a416a2ac6cd4ab4e5cadfb560bc8a66aba388a0167391a65934653ec1908a610fe609abbde319378e4e517bc0ad49daeff311
    version: 0.0.1
  name: workflow:files:oid:fileshare:annotation
  owner: Anonymous
  runtime: py311
  secrets:
    client-id: dummy
    client-secret: dummy
FunctionSchedule:
- cronExpression: 0,30 * * * *
  data:
    WorkflowExtId: wf_oid_files_annotation
    WorkflowVersion: 1
  description: Run every 30 minute
  functionExternalId: fn_workflow_files_oid_fileshare_annotation
  name: every 30 min
Group:
- capabilities:
  - rawAcl:
      actions:
      - READ
      - WRITE
      scope:
        tableScope:
          dbsToTables:
            files_oid_fileshare:
              tables: []
  - filesAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  - extractionConfigsAcl:
      actions:
      - READ
      scope:
        extractionPipelineScope:
          ids:
          - 1
  - extractionRunsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  metadata:
    origin: cdf-project-templates
  name: gp_files_oid_extractor
  sourceId: <change_me>
- capabilities:
  - rawAcl:
      actions:
      - READ
      - WRITE
      scope:
        tableScope:
          dbsToTables:
            files_oid_fileshare:
              tables: []
  - sessionsAcl:
      actions:
      - LIST
      - CREATE
      - DELETE
      scope:
        all: {}
  - assetsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  - filesAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  - extractionConfigsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  - extractionRunsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  - annotationsAcl:
      actions:
      - READ
      - WRITE
      - SUGGEST
      - REVIEW
      scope:
        all: {}
  - entitymatchingAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - labelsAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - functionsAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - transformationsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 42
  metadata:
    origin: cdf-project-templates
  name: gp_files_oid_processing
  sourceId: <change_me>
- capabilities:
  - annotationsAcl:
      actions:
      - READ
      - REVIEW
      scope:
        all: {}
  - labelsAcl:
      actions:
      - READ
      scope:
        all: {}
  - filesAcl:
      actions:
      - READ
      scope:
        datasetScope:
          ids:
          - 42
  metadata:
    origin: cdf-project-templates
  name: gp_files_oid_read
  sourceId: <change_me>
Transformation:
- conflictMode: upsert
  dataSetId: 42
  destination:
    type: files
  destinationOidcCredentials:
    audience: https://bluefield.cognitedata.com
    cdfProjectName: pytest-project
    clientId: dummy
    clientSecret: dummy
    scopes: https://bluefield.cognitedata.com/.default
    tokenUri: dummy
  externalId: tr_files_oid_fileshare_file_metadata
  ignoreNullFields: true
  isPublic: true
  name: files:oid:fileshare:file_metadata
  query: "--\n-- Update file metdata using Transformation\n--\n-- Input data from\
    \ RAW DB table (using example data) and uploaded files\n--\n\nWith \n  root_id\
    \ AS (\n     Select id from _cdf.asset where externalId = 'WMT:VAL'\n  )\nSELECT\n\
    \  file.id                          as id,\n  file.name                      \
    \  as name,\n  file.externalId                  as externalId,\n  meta.source\
    \                      as source,\n  meta.`mime_type`                 as mimeType,\n\
    \  array(root_id.id)                as assetIds,\n  dataset_id('ds_files_oid')\
    \  as dataSetId,\n  map_concat(map(\"doc_type\", meta.doc_type),\n           \
    \ file.metadata)         as metadata\nFROM \n  `files_oid_fileshare`.`files_metadata`\
    \ meta,\n  _cdf.files                             file,\n  root_id\nWHERE \n \
    \ file.name = meta.name\n"
  sourceOidcCredentials:
    audience: https://bluefield.cognitedata.com
    cdfProjectName: pytest-project
    clientId: dummy
    clientSecret: dummy
    scopes: https://bluefield.cognitedata.com/.default
    tokenUri: dummy
TransformationSchedule:
- externalId: tr_files_oid_fileshare_file_metadata
  interval: 7 * * * *
  isPaused: true
Workflow:
- description: Annotation process on files
  externalId: wf_oid_files_annotation
WorkflowVersion:
- version: '1'
  workflowDefinition:
    description: Workflow that annotate files
    tasks:
    - externalId: tr_files_oid_fileshare_file_metadata
      name: Metadata Transformation
      onFailure: abortWorkflow
      parameters:
        transformation:
          concurrencyPolicy: fail
          externalId: tr_files_oid_fileshare_file_metadata
      retries: 3
      timeout: null
      type: transformation
    - dependsOn:
      - externalId: tr_files_oid_fileshare_file_metadata
      description: Function for annotating files
      externalId: fn_context_files_oid_fileshare_annotation
      name: Annotation Function
      onFailure: abortWorkflow
      parameters:
        function:
          data:
            ExtractionPipelineExtId: ep_ctx_files_oid_fileshare_pandid_annotation
          externalId: fn_context_files_oid_fileshare_annotation
      retries: 3
      timeout: null
      type: function
  workflowExternalId: wf_oid_files_annotation
