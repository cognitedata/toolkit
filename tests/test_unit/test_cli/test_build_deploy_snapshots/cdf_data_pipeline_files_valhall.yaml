DataSet:
- description: Asset data for oid
  externalId: ds_asset_oid
  metadata:
    consoleSource: '{"names": ["fileshare"]}'
    rawTables: '[{"databaseName": "asset_oid_fileshare", "tableName": "assets"}]'
    transformations: '[{"externalId": "tr_asset_oid_fileshare_asset_hierarchy", "type":
      "Transformations"}]'
  name: asset:oid
- description: File data for oid
  externalId: ds_files_oid
  metadata:
    consoleSource: '{"names": ["fileshare"]}'
    rawTables: '[{"databaseName": "files_oid_fileshare", "tableName": "files_metadata"}]'
    transformations: '[{"externalId": "tr_files_oid_fileshare_file_metadata", "type":
      "Transformations"}]'
  name: files:oid
Database:
- name: files_oid_fileshare
ExtractionPipeline:
- createdBy: unknown
  dataSetId: 8524431518038195316
  description: Annotation of P&ID documents from file source  oid:fileshare
  documentation: "# Contextualization / Annotation of P&ID Documents\n\nThe P&ID Annotation\
    \ runs as a process in CDF Functions. Process triggers on new and updated documents\
    \ based on update timestamp for document.\n\nExtracts all tags in P&ID that matches\
    \ tags from Asset Hierarchy and creates CDF annotations used for linking found\
    \ objects in document to other resource types in CDF.\n\nDefault configuration\
    \ is provided as part of the deployment, but can be overwritten by configuration\
    \ provided in configuration section below\n```\n # if True, run without writing\
    \ any annotations to CDF\n debug: False\n # if True run annotation on all found\
    \ documents, if False only run on document not updated since last  annotation\n\
    \ runAll: False\n # Number of document from each asset to annotate -1 = All, else\
    \ number is used\n docLimit: -1\n # Data set to read documents / P&ID from + update\
    \ with annotated timestamp when done\n docDataSetExtId: \"ds_files_oid\"\n # In\
    \ document metadata, where is location of property for P&ID type documents\n docTypeMetaCol:\
    \ \"doc_type\"\n # Document type for P&ID type documents\n pAndIdDocType: \"PNID\"\
    \n # List of externalId for root assets to be used for annotation of documents.\
    \ Documents are also extracted based on the root asset ID\n assetRootExtIds: [WMT:VAL]\n\
    \ # Number between 0 and 1, indication of strict the matching should be 1 = only\
    \ exact matches\n matchTreshold: 0.85\n```\n"
  externalId: ep_ctx_files_oid_fileshare_pandid_annotation
  name: ctx:files:oid:fileshare:pandid_annotation
  source: fileshare
- createdBy: unknown
  dataSetId: 8524431518038195316
  description: File source extraction pipeline with configuration for File extractor
    reading data from oid:fileshare
  documentation: "The Cognite File Extractor is a generic file extractor that can\
    \ connect to and extract file from a host of different sources. \n\nThe currently\
    \ supported sources are: \n  * Local files \n  * File Transfer Protocol (FTP)\n\
    \  * Secure File Transfer Protocol (SFTP)\n  * File Transfer Protocol over SSH\
    \ (FTPS)\n  * Documents and files in SharePoint Online \n  * Files in Google Cloud\
    \ Storage \n\n\nThe extractor is provided with template configuration files for\
    \ all mentioned source systems. To set up the file extractor against your source\
    \ system, pick the config template that fits your setup and adjust parameters\
    \ as you see fit.\n"
  externalId: ep_src_files_oid_fileshare
  name: src:files:oid:fileshare
  rawTables:
  - dbName: files_oid_fileshare
    tableName: files_metadata
  source: fileshare
ExtractionPipelineConfig:
- config: "data:\n  debug: False\n  runAll: False\n  docLimit: -1\n  docDataSetExtId:\
    \ \"ds_files_oid\"\n  docTypeMetaCol: \"doc_type\"\n  pAndIdDocType: \"PNID\"\n\
    \  assetRootExtIds: [WMT:VAL]\n  matchThreshold: 0.85\n \n"
  externalId: ep_ctx_files_oid_fileshare_pandid_annotation
- config: "logger:\n  # Logging to console/terminal. Remove or comment out to disable\
    \ terminal\n  # logging\n  console:\n    level: INFO\n\n  # Logging to file. Include\
    \ to enable file logging\n  file:\n    level: INFO\n    path: \"c:/tmp/file.log\"\
    \n\ncognite:\n  # Data set to attach uploaded files to. Either use CDF IDs (integers)\
    \ or\n  # user-given external ids (strings)\n  data-set:\n    external-id: ds-files:valhall\n\
    \n# Information about files to extract\nfiles:\n  extensions:\n    - .pdf\n\n\
    \  # Information about file provider\n  file-provider:\n    type: local\n\n  \
    \  # For local files: Absolute or relative path to directory to watch\n    path:\
    \ c:/tmp/files"
  externalId: ep_src_files_oid_fileshare
Function:
- cpu: 0.6
  description: Contextualization of P&ID files creating annotations
  envVars:
    CDF_ENV: dev
    ENV_TYPE: dev
  externalId: fn_context_files_oid_fileshare_annotation
  fileId: -1
  functionPath: handler.py
  metadata:
    cognite-toolkit-hash: /=d7107f17;__init__.py=e3b0c442;config.py=f8bc25c0;constants.py=0b7ca6c0;handler.py=f77bd6e4;pipeline.py=577e53f0;requirements.txt=8d59ed94
    version: 0.0.1
  name: context:files:oid:fileshare:annotation
  owner: Anonymous
  runtime: py311
- cpu: 0.6
  description: Workflow scheduler for Contextualization of P&ID files creating annotations
  envVars:
    CDF_ENV: dev
    ENV_TYPE: dev
  externalId: fn_workflow_files_oid_fileshare_annotation
  fileId: -1
  functionPath: handler.py
  metadata:
    cdf-toolkit-secret-hash: 713508150112ea25b1d8587dab587e20a3cd564511918a24c3e59e48dd81d3193b27abe3f74fff394dae118da13a162908b93c77232301a1bb3d81366d963b25
    cognite-toolkit-hash: /=c59aa6c7;__init__.py=e3b0c442;config.py=fb0a951c;handler.py=959ae0cd;pipeline.py=c1ab43e9;requirements.txt=ea7478ac
    version: 0.0.1
  name: workflow:files:oid:fileshare:annotation
  owner: Anonymous
  runtime: py311
  secrets:
    client-id: dummy-123
    client-secret: dummy-secret
FunctionSchedule:
- cronExpression: 0,30 * * * *
  data:
    WorkflowExtId: wf_oid_files_annotation
    WorkflowVersion: 1
  description: 'Run every 30 minute cdf-auth: def5e2e0'
  functionExternalId: fn_workflow_files_oid_fileshare_annotation
  name: every 30 min
Group:
- capabilities:
  - rawAcl:
      actions:
      - READ
      - WRITE
      scope:
        tableScope:
          dbsToTables:
            files_oid_fileshare:
              tables: []
  - filesAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  - extractionConfigsAcl:
      actions:
      - READ
      scope:
        extractionPipelineScope:
          ids:
          - 13081016686738274100
  - extractionRunsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  metadata:
    origin: cdf-project-templates
  name: gp_files_oid_extractor
  sourceId: <change_me>
- capabilities:
  - rawAcl:
      actions:
      - READ
      - WRITE
      scope:
        tableScope:
          dbsToTables:
            files_oid_fileshare:
              tables: []
  - sessionsAcl:
      actions:
      - LIST
      - CREATE
      - DELETE
      scope:
        all: {}
  - assetsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 5942118280324318086
  - filesAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  - extractionConfigsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  - extractionRunsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  - annotationsAcl:
      actions:
      - READ
      - WRITE
      - SUGGEST
      - REVIEW
      scope:
        all: {}
  - entitymatchingAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - labelsAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - functionsAcl:
      actions:
      - READ
      - WRITE
      scope:
        all: {}
  - transformationsAcl:
      actions:
      - READ
      - WRITE
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  metadata:
    origin: cdf-project-templates
  name: gp_files_oid_processing
  sourceId: <change_me>
- capabilities:
  - annotationsAcl:
      actions:
      - READ
      - REVIEW
      scope:
        all: {}
  - labelsAcl:
      actions:
      - READ
      scope:
        all: {}
  - filesAcl:
      actions:
      - READ
      scope:
        datasetScope:
          ids:
          - 8524431518038195316
  metadata:
    origin: cdf-project-templates
  name: gp_files_oid_read
  sourceId: <change_me>
Table:
- createdTime: 1
  name: files_metadata
Transformation:
- conflictMode: upsert
  dataSetId: 8524431518038195316
  destination:
    type: files
  destinationOidcCredentials:
    audience: https://bluefield.cognitedata.com
    cdfProjectName: pytest-project
    clientId: dummy-123
    clientSecret: dummy-secret
    scopes: https://bluefield.cognitedata.com/.default
    tokenUri: https://login.microsoftonline.com/dummy-domain/oauth2/v2.0/token
  externalId: tr_files_oid_fileshare_file_metadata
  ignoreNullFields: true
  isPublic: true
  name: files:oid:fileshare:file_metadata
  query: "-- cdf-auth: f5da81f3\n--\n-- Update file metdata using Transformation\n\
    --\n-- Input data from RAW DB table (using example data) and uploaded files\n\
    --\n\nWith \n  root_id AS (\n     Select id from _cdf.asset where externalId =\
    \ 'WMT:VAL'\n  )\nSELECT\n  file.id                          as id,\n  file.name\
    \                        as name,\n  file.externalId                  as externalId,\n\
    \  meta.source                      as source,\n  meta.`mime_type`           \
    \      as mimeType,\n  array(root_id.id)                as assetIds,\n  dataset_id('ds_files_oid')\
    \  as dataSetId,\n  map_concat(map(\"doc_type\", meta.doc_type),\n           \
    \ file.metadata)         as metadata\nFROM \n  `files_oid_fileshare`.`files_metadata`\
    \ meta,\n  _cdf.files                             file,\n  root_id\nWHERE \n \
    \ file.name = meta.name\n"
  sourceOidcCredentials:
    audience: https://bluefield.cognitedata.com
    cdfProjectName: pytest-project
    clientId: dummy-123
    clientSecret: dummy-secret
    scopes: https://bluefield.cognitedata.com/.default
    tokenUri: https://login.microsoftonline.com/dummy-domain/oauth2/v2.0/token
TransformationSchedule:
- externalId: tr_files_oid_fileshare_file_metadata
  interval: 7 * * * *
  isPaused: true
Workflow:
- dataSetId: 8524431518038195316
  description: Annotation process on files
  externalId: wf_oid_files_annotation
WorkflowVersion:
- version: '1'
  workflowDefinition:
    description: Workflow that annotate files
    tasks:
    - externalId: tr_files_oid_fileshare_file_metadata
      name: Metadata Transformation
      onFailure: abortWorkflow
      parameters:
        transformation:
          concurrencyPolicy: fail
          externalId: tr_files_oid_fileshare_file_metadata
          useTransformationCredentials: false
      retries: 3
      type: transformation
    - dependsOn:
      - externalId: tr_files_oid_fileshare_file_metadata
      description: Function for annotating files
      externalId: fn_context_files_oid_fileshare_annotation
      name: Annotation Function
      onFailure: abortWorkflow
      parameters:
        function:
          data:
            ExtractionPipelineExtId: ep_ctx_files_oid_fileshare_pandid_annotation
          externalId: fn_context_files_oid_fileshare_annotation
      retries: 3
      type: function
  workflowExternalId: wf_oid_files_annotation
deleted: {}
